{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d0dea3",
   "metadata": {},
   "source": [
    "# PDF Ingestion Example\n",
    "\n",
    "This notebook demonstrates how to use the PDF ingestion utilities to process a PDF file and store the chunks in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403ac348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from utils.model_costs import ModelUsageAsync\n",
    "from utils.openai_calls import call_openai_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97cc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these to your existing imports\n",
    "from utils.pdf_ingestion import ingest_pdf, PDFChunk, PDFDocument\n",
    "from utils.vector_store import VectorStore, get_query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb247a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # .env should be in the root folder (sibling of this notebook)\n",
    "\n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_PROJECT_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f9cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PDF: basic_laws.pdf\n",
      "Extracted 170 pages\n",
      "Created 169 chunks\n",
      "Embedding tokens used: 136620\n",
      "Embedding cost: $0.0027324\n",
      "\n",
      "Sample chunk:\n",
      "Page: 1\n",
      "Characters: 0 to 231\n",
      "Tokens: 53\n",
      "Text excerpt: 2016 edition\n",
      "BASIC\n",
      "LAWS\n",
      "and AUTHORITIES of the NATIONAL ARCHIVES\n",
      "and RECORDS ADMINISTR ATION\n",
      "Office of General Counsel\n",
      "National Archives and Records Administration\n",
      "www.archives.gov\n",
      "Additional material...\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = \"basic_laws.pdf\"\n",
    "\n",
    "# Create usage tracker\n",
    "pdf_ingestion_usage = ModelUsageAsync()\n",
    "\n",
    "# Process the PDF: extract text, chunk, and embed\n",
    "pdf_doc, chunks = await ingest_pdf(\n",
    "    pdf_path=pdf_path,\n",
    "    openai_client=openai_client,\n",
    "    target_chunk_tokens=350,  # As specified in the tech spec\n",
    "    chunk_overlap=0.3,        # 30% overlap as specified\n",
    "    embedding_model=\"text-embedding-3-small\",\n",
    "    llm_usage=pdf_ingestion_usage\n",
    ")\n",
    "\n",
    "# Print some stats\n",
    "print(f\"Processed PDF: {pdf_doc.filename}\")\n",
    "print(f\"Extracted {len(pdf_doc.page_texts)} pages\")\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"Embedding tokens used: {await pdf_ingestion_usage.get_tokens_used()}\")\n",
    "print(f\"Embedding cost: ${await pdf_ingestion_usage.get_cost()}\")\n",
    "\n",
    "# Display first chunk as example\n",
    "if chunks:\n",
    "    first_chunk = chunks[0]\n",
    "    print(\"\\nSample chunk:\")\n",
    "    print(f\"Page: {first_chunk.page_index + 1}\")\n",
    "    print(f\"Characters: {first_chunk.char_start} to {first_chunk.char_end}\")\n",
    "    print(f\"Tokens: {first_chunk.tokens}\")\n",
    "    print(f\"Text excerpt: {first_chunk.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3639af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 169 chunks to vector store\n"
     ]
    }
   ],
   "source": [
    "# Create vector store and add chunks\n",
    "vector_store = VectorStore(embedding_dim=1536)  # dimension for text-embedding-3-small\n",
    "vector_store.add_chunks(chunks)\n",
    "\n",
    "print(f\"Added {len(chunks)} chunks to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89259060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How and when can the President dispose of Presidential records?\n",
      "Retrieved 6 relevant chunks\n",
      "\n",
      "Chunk 1 (Page 50):\n",
      "(4) The term “Archivist” means the Archivist of the mittees at least 60 calendar days of continuous ...\n",
      "\n",
      "Chunk 2 (Page 122):\n",
      "EXECUTIVE ORDER 13489—\n",
      "PRESIDENTIAL RECORDS\n",
      "By the authority vested in me as President by the Sec. 2...\n",
      "\n",
      "Chunk 3 (Page 51):\n",
      "(3) The Archivist is authorized to dispose of such Pres- (ii) the expiration of the duration specifi...\n",
      "\n",
      "Chunk 4 (Page 42):\n",
      "(ii) any personnel with appropriate security clearances 44 U.S.C. § 2111 NOTE\n",
      "of a Federal contracto...\n",
      "\n",
      "Chunk 5 (Page 68):\n",
      "the end of the periods specified, have sufficient admin- When the Archivist and the head of the agen...\n",
      "\n",
      "Chunk 6 (Page 5):\n",
      "FEDERAL REGISTER AND THE CODE OF FEDERAL REGULATIONS 11\n",
      "§ 1501. Definitions 12\n",
      "§ 1502. Custody and p...\n",
      "\n",
      "Embedding tokens used: 11\n",
      "Embedding cost: $2.2e-07\n"
     ]
    }
   ],
   "source": [
    "# Define some test questions\n",
    "test_questions = [\n",
    "    \"How and when can the President dispose of Presidential records?\",\n",
    "    \"What proivsions are teh Vice-Presidential records subject to?\",\n",
    "    # Add more relevant questions about your PDF content\n",
    "]\n",
    "\n",
    "# Function to retrieve and display results\n",
    "async def query_document(question):\n",
    "    print(f\"\\nQuery: {question}\")\n",
    "    \n",
    "    # Track usage\n",
    "    query_usage = ModelUsageAsync()\n",
    "    \n",
    "    # Get embedding for query\n",
    "    query_embedding = await get_query_embedding(\n",
    "        query=question,\n",
    "        openai_client=openai_client,\n",
    "        embedding_model=\"text-embedding-3-small\",\n",
    "        llm_usage=query_usage\n",
    "    )\n",
    "    \n",
    "    # Retrieve relevant chunks with MMR for diversity\n",
    "    retrieved_chunks = vector_store.mmr_search(\n",
    "        query_embedding=query_embedding,\n",
    "        k=6,  # As specified in tech spec\n",
    "        lambda_param=0.7  # Balance between relevance and diversity\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "    \n",
    "    # Display retrieved chunks\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        print(f\"\\nChunk {i+1} (Page {chunk.page_index + 1}):\")\n",
    "        # Display a preview of the text (first 100 characters)\n",
    "        print(f\"{chunk.text[:100]}...\")\n",
    "    \n",
    "    print(f\"\\nEmbedding tokens used: {await query_usage.get_tokens_used()}\")\n",
    "    print(f\"Embedding cost: ${await query_usage.get_cost()}\")\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "# Test with the first question\n",
    "retrieved_chunks = await query_document(test_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38869c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating answer for: How and when can the President dispose of Presidential records?\n",
      "\n",
      "Answer:\n",
      "The Presidential Records Act authorizes the President, during his term of office, to discard his own records that “no longer have administrative, historical, informational, or evidentiary value” so long as he first secures the Archivist’s written views and a statement that no action under § 2203(e) will be taken:  \n",
      "“During the President’s term of office, the President may dispose of those Presidential records … if (1) the President obtains the views, in writing, of the Archivist concerning the proposed disposal of such Presidential records; and (2) the Archivist states that the Archivist does not intend to take any action under subsection (e) of this section.” (page 41)  \n",
      "\n",
      "If the Archivist does object (i.e., notifies the President of an intent to take action under § 2203(e)), the President may still dispose of the records only after providing copies of the disposal schedule to the appropriate Congressional committees “at least 60 calendar days of continuous session of Congress in advance of the proposed disposal date.” (page 41)\n",
      "\n",
      "Tokens used: 7835\n",
      "Answer cost: $0.0165979\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt for question answering with citations\n",
    "QA_WITH_CITATIONS_PROMPT = \"\"\"\n",
    "Task: Answer the user's question based ONLY on the provided context. \n",
    "Include verbatim quotes from the context to support your answer.\n",
    "Format your answer with cited text in quotes and include the page number in parentheses.\n",
    "\n",
    "User question: {user_question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Your answer must:\n",
    "1. Only contain information present in the context\n",
    "2. Include at least 2 direct quotes from the context\n",
    "3. Specify the page number for each quote in parentheses\n",
    "4. Be concise and focused on the question\n",
    "\"\"\"\n",
    "\n",
    "async def answer_with_citations(question, retrieved_chunks):\n",
    "    print(f\"\\nGenerating answer for: {question}\")\n",
    "    \n",
    "    # Format context from chunks\n",
    "    context_parts = []\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        context_parts.append(f\"Page {chunk.page_index + 1}:\\n{chunk.text}\\n\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Create message history\n",
    "    message_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert assistant that answers questions based solely on provided context.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": QA_WITH_CITATIONS_PROMPT.format(\n",
    "                user_question=question,\n",
    "                context=context\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Track usage\n",
    "    answer_usage = ModelUsageAsync()\n",
    "    \n",
    "    # Call LLM to generate answer\n",
    "    model_response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",  # First call with o4-mini as specified\n",
    "        messages=message_history,\n",
    "        reasoning_effort=\"high\",\n",
    "        llm_usage=answer_usage\n",
    "    )\n",
    "    \n",
    "    answer = model_response.choices[0].message.content\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{answer}\")\n",
    "    print(f\"\\nTokens used: {await answer_usage.get_tokens_used()}\")\n",
    "    print(f\"Answer cost: ${await answer_usage.get_cost()}\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Generate answer for the first question\n",
    "answer = await answer_with_citations(test_questions[0], retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84851f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What proivsions are teh Vice-Presidential records subject to?\n",
      "Retrieved 6 relevant chunks\n",
      "\n",
      "Chunk 1 (Page 52):\n",
      "(2) Nothing in this Act shall be construed to confirm, shall be available to such former President o...\n",
      "\n",
      "Chunk 2 (Page 5):\n",
      "FEDERAL REGISTER AND THE CODE OF FEDERAL REGULATIONS 11\n",
      "§ 1501. Definitions 12\n",
      "§ 1502. Custody and p...\n",
      "\n",
      "Chunk 3 (Page 42):\n",
      "(ii) any personnel with appropriate security clearances 44 U.S.C. § 2111 NOTE\n",
      "of a Federal contracto...\n",
      "\n",
      "Chunk 4 (Page 122):\n",
      "EXECUTIVE ORDER 13489—\n",
      "PRESIDENTIAL RECORDS\n",
      "By the authority vested in me as President by the Sec. 2...\n",
      "\n",
      "Chunk 5 (Page 49):\n",
      "§ 2120. ONLINE ACCESS OF FOUNDING (3) Thomas Jefferson;\n",
      "FATHERS DOCUMENTS (4) Benjamin Franklin;\n",
      "The...\n",
      "\n",
      "Chunk 6 (Page 51):\n",
      "(3) The Archivist is authorized to dispose of such Pres- (ii) the expiration of the duration specifi...\n",
      "\n",
      "Embedding tokens used: 15\n",
      "Embedding cost: $3e-07\n",
      "\n",
      "Generating answer for: What proivsions are teh Vice-Presidential records subject to?\n",
      "\n",
      "Answer:\n",
      "“Vice-Presidential records shall be subject to the provisions of this chapter in the same manner as Presidential records.” (page 52)\n",
      "\n",
      "“The duties and responsibilities of the Vice President, with respect to Vice-Presidential records, shall be the same as the duties and responsibilities of the President under this chapter, except section 2208, with respect to Presidential records.” (page 52)\n",
      "\n",
      "Tokens used: 6647\n",
      "Answer cost: $0.011647900000000001\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run through all test questions\n",
    "for question in test_questions[1:]:  # Skip the first one we already did\n",
    "    retrieved_chunks = await query_document(question)\n",
    "    answer = await answer_with_citations(question, retrieved_chunks)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
