{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Highlighting Example\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Process a PDF document (extract, chunk, and embed text)\n",
    "2. Answer a question using the processed PDF\n",
    "3. Highlight the cited text in the original PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from utils.model_costs import ModelUsageAsync\n",
    "from utils.pdf_ingestion import ingest_pdf\n",
    "from utils.vector_store import VectorStore, get_query_embedding\n",
    "from utils.openai_calls import call_openai_structured\n",
    "from utils.pdf_highlighter import highlight_pdf_with_citations\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_PROJECT_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process the PDF Document\n",
    "\n",
    "First, we'll process the PDF to extract text, chunk it, and generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to your PDF file\n",
    "pdf_path = \"patent.pdf\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"PDF file not found: {pdf_path}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    # List PDF files in current directory\n",
    "    pdf_files = [f for f in os.listdir() if f.endswith('.pdf')]\n",
    "    if pdf_files:\n",
    "        print(\"Available PDF files:\")\n",
    "        for pdf in pdf_files:\n",
    "            print(f\"- {pdf}\")\n",
    "        \n",
    "        # Use the first available PDF\n",
    "        pdf_path = pdf_files[0]\n",
    "        print(f\"Using: {pdf_path}\")\n",
    "    else:\n",
    "        print(\"No PDF files found in current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PDF: patent.pdf\n",
      "Total pages: 25\n",
      "Total chunks: 142\n",
      "Embedding tokens used: 41750\n",
      "Embedding cost: $0.000835\n"
     ]
    }
   ],
   "source": [
    "# Initialize usage tracker\n",
    "pdf_usage = ModelUsageAsync()\n",
    "\n",
    "# Process the PDF: extract text, chunk, and embed\n",
    "pdf_doc, chunks = await ingest_pdf(\n",
    "    pdf_path=pdf_path,\n",
    "    openai_client=openai_client,\n",
    "    target_chunk_tokens=350,  # ~350 tokens per chunk as specified in tech spec\n",
    "    chunk_overlap=0.3,        # 30% overlap as specified\n",
    "    embedding_model=\"text-embedding-3-small\",\n",
    "    llm_usage=pdf_usage\n",
    ")\n",
    "\n",
    "# Print stats\n",
    "print(f\"Processed PDF: {pdf_doc.filename}\")\n",
    "print(f\"Total pages: {len(pdf_doc.page_texts)}\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Embedding tokens used: {await pdf_usage.get_tokens_used()}\")\n",
    "print(f\"Embedding cost: ${await pdf_usage.get_cost()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Vector Store and Search\n",
    "\n",
    "Now we'll create a vector store with the document chunks for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 142 chunks to vector store\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "vector_store = VectorStore(embedding_dim=1536)  # dimension for text-embedding-3-small\n",
    "vector_store.add_chunks(chunks)\n",
    "print(f\"Added {len(chunks)} chunks to vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Answer Questions with Citations\n",
    "\n",
    "Now we'll create a function to answer questions with verbatim citations from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_question(question: str):\n",
    "    \"\"\"Answer a question with citations from the PDF.\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    # Get embedding for query\n",
    "    query_usage = ModelUsageAsync()\n",
    "    query_embedding = await get_query_embedding(\n",
    "        query=question,\n",
    "        openai_client=openai_client,\n",
    "        embedding_model=\"text-embedding-3-small\",\n",
    "        llm_usage=query_usage\n",
    "    )\n",
    "    \n",
    "    # Retrieve relevant chunks with Maximum Marginal Relevance for diversity\n",
    "    retrieved_chunks = vector_store.mmr_search(\n",
    "        query_embedding=query_embedding,\n",
    "        k=6,  # Get top-6 chunks as specified in tech spec\n",
    "        lambda_param=0.7  # Balance between relevance and diversity\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "    \n",
    "    # Create context from chunks\n",
    "    context_parts = []\n",
    "    for chunk in retrieved_chunks:\n",
    "        context_parts.append(f\"Page {chunk.page_index + 1}:\\n{chunk.text}\\n\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Create QA prompt\n",
    "    qa_prompt = \"\"\"\n",
    "    Answer the question based ONLY on the provided context.\n",
    "    Include verbatim quotes from the context to support your answer.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Format your response as a JSON object with these fields:\n",
    "    1. \"answer\": Your detailed answer to the question\n",
    "    2. \"citations\": A list of citation objects, each with:\n",
    "       - \"page\": The page number (integer)\n",
    "       - \"text\": The exact quote from that page (string)\n",
    "    \n",
    "    Example format:\n",
    "    {{\"answer\": \"Your answer here...\", \"citations\": [{{\"page\": 1, \"text\": \"Exact quote from page 1\"}}, {{\"page\": 2, \"text\": \"Another quote from page 2\"}}]}}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message history\n",
    "    message_history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert assistant that answers questions based solely on provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": qa_prompt.format(question=question, context=context)}\n",
    "    ]\n",
    "    \n",
    "    # Get answer with o4-mini as specified in tech spec\n",
    "    answer_usage = ModelUsageAsync()\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=message_history,\n",
    "        reasoning_effort=\"high\",\n",
    "        llm_usage=answer_usage\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    # Parse JSON response\n",
    "    try:\n",
    "        # Look for JSON object in the response\n",
    "        json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if json_match:\n",
    "            result = json.loads(json_match.group(0))\n",
    "        else:\n",
    "            # Fallback parsing if not properly formatted\n",
    "            result = {\"answer\": content, \"citations\": []}\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON response. Using raw content.\")\n",
    "        result = {\"answer\": content, \"citations\": []}\n",
    "    \n",
    "    # Calculate total usage\n",
    "    total_tokens = await query_usage.get_tokens_used() + await answer_usage.get_tokens_used()\n",
    "    total_cost = await query_usage.get_cost() + await answer_usage.get_cost()\n",
    "    \n",
    "    print(f\"Total tokens used: {total_tokens}\")\n",
    "    print(f\"Total cost: ${total_cost}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ask a Question\n",
    "\n",
    "Let's ask a question about the PDF and get an answer with citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: A method to develop a search engine rank for object-source pairs within a corpus of published documents, the method comprising: semantically identifying, by an evaluation module, objects and source values contained within the corpus of published documents, wherein each source value is a name of an organization, and wherein the objects and source values each include one or more words identified within a published document in the corpus of published documents tying, by the evaluation module, each instance of a first object throughout the corpus of published documents to a source value based on: identifying a first instance of the first object in a first published document of the corpus of published documents\n",
      "Retrieved 6 relevant chunks\n",
      "Total tokens used: 3617\n",
      "Total cost: $0.008659479999999999\n",
      "\n",
      "Answer:\n",
      "The provided context does not disclose any method for “develop[ing] a search engine rank for object-source pairs” nor any evaluation module that semantically identifies objects and organization names and ties each instance of a first object to a source value. The excerpts only describe selecting source objects/documents and extracting object names and attribute-value pairs; there is no mention of ranking object-source pairs or an evaluation module as claimed.\n",
      "\n",
      "Citations:\n",
      "Citation 1 - Page 24:\n",
      "\"selecting a source object from a plurality of objects stored in a fact repository, the source object having an object name and an attribute-value pair;\"\n",
      "\n",
      "Citation 2 - Page 19:\n",
      "\"The importer 108 identifies 330 a title pattern and a contextual pattern based on the source document and the source object.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample question about the document\n",
    "question = \"A method to develop a search engine rank for object-source pairs within a corpus of published documents, the method comprising: semantically identifying, by an evaluation module, objects and source values contained within the corpus of published documents, wherein each source value is a name of an organization, and wherein the objects and source values each include one or more words identified within a published document in the corpus of published documents tying, by the evaluation module, each instance of a first object throughout the corpus of published documents to a source value based on: identifying a first instance of the first object in a first published document of the corpus of published documents\"\n",
    "\n",
    "# Get answer with citations\n",
    "result = await answer_question(question)\n",
    "\n",
    "# Display the answer\n",
    "print(\"\\nAnswer:\")\n",
    "print(result[\"answer\"])\n",
    "\n",
    "# Display citations\n",
    "print(\"\\nCitations:\")\n",
    "for i, citation in enumerate(result[\"citations\"]):\n",
    "    print(f\"Citation {i+1} - Page {citation['page']}:\")\n",
    "    print(f'\"{citation[\"text\"]}\"')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Highlight Citations in the PDF\n",
    "\n",
    "Now we'll use our PDF highlighter to display the document with highlighted citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening PDF: patent.pdf\n",
      "The PDF viewer will open in a new window.\n",
      "- The answer is displayed in the top-right panel\n",
      "- Citations are listed below the answer\n",
      "- Click on a citation to navigate to that page\n",
      "- Citations are highlighted in bright yellow in the document\n",
      "Found 0 citations for page 1\n",
      "Page 1 has 3170 characters of text\n",
      "Found 1 citations for page 24\n",
      "Citation page numbers: [24]\n",
      "Page 24 has 6494 characters of text\n",
      "Looking for citation: selecting a source object from a plurality of obje...\n",
      "Successfully highlighted using _highlight_with_exact_match\n",
      "Found 1 citations for page 19\n",
      "Citation page numbers: [19]\n",
      "Page 19 has 7723 characters of text\n",
      "Looking for citation: The importer 108 identifies 330 a title pattern an...\n",
      "Successfully highlighted using _highlight_with_simplified_match\n",
      "Found 1 citations for page 24\n",
      "Citation page numbers: [24]\n",
      "Page 24 has 6494 characters of text\n",
      "Looking for citation: selecting a source object from a plurality of obje...\n",
      "Successfully highlighted using _highlight_with_exact_match\n"
     ]
    }
   ],
   "source": [
    "from utils.pdf_highlighter import highlight_pdf_with_citations\n",
    "\n",
    "print(f\"Opening PDF: {pdf_path}\")\n",
    "print(\"The PDF viewer will open in a new window.\")\n",
    "print(\"- The answer is displayed in the top-right panel\")\n",
    "print(\"- Citations are listed below the answer\")\n",
    "print(\"- Click on a citation to navigate to that page\")\n",
    "print(\"- Citations are highlighted in bright yellow in the document\")\n",
    "\n",
    "# Open the PDF with the improved highlighter\n",
    "highlight_pdf_with_citations(pdf_path, result[\"citations\"], result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
